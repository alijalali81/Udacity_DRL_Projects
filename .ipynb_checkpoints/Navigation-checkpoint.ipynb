{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Navigation\n",
    "\n",
    "---\n",
    "\n",
    "You are welcome to use this coding environment to train your agent for the project.  Follow the instructions below to get started!\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "Run the next code cell to install a few packages.  This line will take a few minutes to run!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install ./python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The environment is already saved in the Workspace and can be accessed at the file path provided below.  Please run the next code cell without making any changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: BananaBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 37\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: discrete\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "\n",
    "# please do not modify the line below\n",
    "env = UnityEnvironment(file_name=\"/data/Banana_Linux_NoVis/Banana.x86_64\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Examine the State and Action Spaces\n",
    "\n",
    "Run the code cell below to print some information about the environment.\n",
    "\n",
    "The agent can take 4 actions: forward, backward, right, and left.\n",
    "\n",
    "The environment has 37 states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 1\n",
      "Number of actions: 4\n",
      "States look like: [ 1.          0.          0.          0.          0.84408134  0.          0.\n",
      "  1.          0.          0.0748472   0.          1.          0.          0.\n",
      "  0.25755     1.          0.          0.          0.          0.74177343\n",
      "  0.          1.          0.          0.          0.25854847  0.          0.\n",
      "  1.          0.          0.09355672  0.          1.          0.          0.\n",
      "  0.31969345  0.          0.        ]\n",
      "States have length: 37\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents in the environment\n",
    "print('Number of agents:', len(env_info.agents))\n",
    "\n",
    "# number of actions\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Number of actions:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "state = env_info.vector_observations[0]\n",
    "print('States look like:', state)\n",
    "state_size = len(state)\n",
    "print('States have length:', state_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Take Random Actions in the Environment\n",
    "\n",
    "In the next code cell, you will learn how to use the Python API to control the agent and receive feedback from the environment.\n",
    "\n",
    "Note that **in this coding environment, you will not be able to watch the agent while it is training**, and you should set `train_mode=True` to restart the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.0\n"
     ]
    }
   ],
   "source": [
    "env_info = env.reset(train_mode=True)[brain_name] # reset the environment\n",
    "state = env_info.vector_observations[0]            # get the current state\n",
    "score = 0                                          # initialize the score\n",
    "while True:\n",
    "    action = np.random.randint(action_size)        # select an action\n",
    "    env_info = env.step(action)[brain_name]        # send the action to the environment\n",
    "    next_state = env_info.vector_observations[0]   # get the next state\n",
    "    reward = env_info.rewards[0]                   # get the reward\n",
    "    done = env_info.local_done[0]                  # see if episode has finished\n",
    "    score += reward                                # update the score\n",
    "    state = next_state                             # roll over the state to next time step\n",
    "    if done:                                       # exit loop if episode finished\n",
    "        break\n",
    "    \n",
    "print(\"Score: {}\".format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When finished, you can close the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. It's Your Turn!\n",
    "\n",
    "Now it's your turn to train your own agent to solve the environment!  A few **important notes**:\n",
    "- When training the environment, set `train_mode=True`, so that the line for resetting the environment looks like the following:\n",
    "```python\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "```\n",
    "- To structure your work, you're welcome to work directly in this Jupyter notebook, or you might like to start over with a new file!  You can see the list of files in the workspace by clicking on **_Jupyter_** in the top left corner of the notebook.\n",
    "- In this coding environment, you will not be able to watch the agent while it is training.  However, **_after training the agent_**, you can download the saved model weights to watch the agent on your own machine! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Math, Latex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DQN Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reinforcement learning is known to be unstable or even to diverge. This\n",
    "instability has several causes: the correlations present in the sequence\n",
    "of observations, thefact that small updates to $Q$ may significantly change\n",
    "the policy and therefore change the data distribution, and the correlations\n",
    "between the action-values $(Q)$ and the target value: $r+\\gamma_{max} Q(s\\prime,a\\prime)$.\n",
    "\n",
    "Deep Q Network (DQN) developed to address the above mentioned problems in RL. DQN is off-policy reinforcement learning which is a variation of Q-learning algorithm. In DQN deep  neural network is used to approximate the optimal action-value function which is the maximum sum of rewards $r_t$ discounted by $\\gamma$ at each timestep $t$, achievable by a behaviour policy $\\pi=P(a|s)$, after making an\n",
    "observation ($s$) and taking an action ($a$) (see Methods)And it uses replay memory.\n",
    "\n",
    "\n",
    "<br>Replay memory can break the correlation of samples, because the training data doesn't depend on time.<br>In NN training, DQN uses Mean Squared Error(or Huber loss) for training loss.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import collections\n",
    "import pickle\n",
    "#from unityagents import UnityEnvironment\n",
    "#import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the Deep Q Network agent (DQN) as a class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_r=0.001 # Default Learning Rate \n",
    "d_r=0.95 # Default Discount Rate\n",
    "r_s=3000 # Replay Memory Buffer\n",
    "b_s=64 # Default Batch Size\n",
    "\n",
    "class DQNAgt:\n",
    "    def __init__(self, state_size, action_size, build_network, lr=l_r, dr=d_r,\n",
    "                 rs=r_s, bs=b_s):\n",
    "        \n",
    "        torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "        torch.cuda.set_device(0)\n",
    "        \n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.lr = lr\n",
    "        self.batch_size = bs\n",
    "        self.discount_rate = dr\n",
    "        self.replay_memory = collections.deque(maxlen=r_s)\n",
    "        self.running_network = build_network.cuda()\n",
    "        self.target_network = build_network.cuda()\n",
    "\n",
    "        self.running_optimizer = torch.optim.RMSprop(self.running_network.parameters(), lr=self.lr)\n",
    "        self.update_target_network()\n",
    "\n",
    "    def update_target_network(self):\n",
    "        self.target_network.load_state_dict(self.running_network.state_dict())\n",
    "    \n",
    "    def optimize(self, x, y):\n",
    "        loss_func = torch.nn.SmoothL1Loss() \n",
    "        #loss_func = torch.nn.MSELoss()\n",
    "        loss = loss_func(x, y)\n",
    "        self.running_optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.runner_optimizer.step()\n",
    "    \n",
    "    def get_action(self, state):\n",
    "        self.running_network.eval()\n",
    "        return np.argmax(self.running_network(torch.tensor([state], requires_grad=False, dtype=torch.float32)).cpu().detach().numpy())\n",
    "    \n",
    "    def max_q(self, state):\n",
    "        self.running_network.eval()\n",
    "        return np.max(self.running_network(torch.tensor([state], requires_grad=False, dtype=torch.float32)).cpu().detach().numpy())\n",
    "        \n",
    "    \n",
    "    def append_replay_memory(self, state, action, reward, next_state, done):\n",
    "        self.replay_memory.append([state, action, reward, next_state, done])\n",
    "    \n",
    "    def get_batch(self):\n",
    "        batch = random.sample(self.replay_memory, self.batch_size)\n",
    "        state, next_state = np.empty([self.batch_size, self.state_size*4]), np.empty([self.batch_size, self.state_size*4])\n",
    "        action, reward, done = [], [], []\n",
    "        \n",
    "        for i in range(self.batch_size):\n",
    "            state[i] = batch[i][0]\n",
    "            action.append(batch[i][1])\n",
    "            reward.append(batch[i][2])\n",
    "            next_state[i] = batch[i][3]\n",
    "            done.append(batch[i][4])\n",
    "        return state, action, reward, next_state, done\n",
    "    \n",
    "    def train(self):\n",
    "        self.running_network.train()\n",
    "        s, a, r, s_n, d = self.get_batch()\n",
    "        tensor_s = torch.tensor(s, requires_grad=False, dtype=torch.float32)\n",
    "        tensor_s_n = torch.tensor(s_n, requires_grad=False, dtype=torch.float32)\n",
    "        target_q = self.target_network(tensor_s_n).cpu()\n",
    "        runner_q = self.running_network(tensor_s).cpu()\n",
    "        \n",
    "        update_target = np.empty([self.state_size, self.action_size])\n",
    "        update_target = target_q.detach().numpy()\n",
    "        \n",
    "        q = torch.tensor(target_q)\n",
    "        for i in range(self.batch_size):\n",
    "            if d[i] is True:\n",
    "                update_target[i][a[i]] = r[i]\n",
    "            else:\n",
    "                update_target[i][a[i]] = r[i] + self.discount_rate * torch.max(target_q[i])\n",
    "        \n",
    "\n",
    "        current = torch.tensor(runner_q, requires_grad=True, dtype=torch.float32)\n",
    "        target = torch.tensor(update_target, requires_grad=False, dtype=torch.float32)\n",
    "        \n",
    "        loss_func = torch.nn.SmoothL1Loss()\n",
    "        loss = loss_func(current, target)\n",
    "        self.running_optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.running_optimizer.step()\n",
    "    \n",
    "    def save_model(self, p):\n",
    "        torch.save(self.running_network.state_dict(), p)\n",
    "    \n",
    "    def restore_model(self, p):\n",
    "        self.running_network.load_state_dict(torch.load(p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Designing the network\n",
    "Designing a deep neural network with 3 hidden layers: [64,64,32]\n",
    "Initialization: Xavier\n",
    "Regularization: Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "dqn_size=[action_size*state_size,64,64,32,action_size]\n",
    "\n",
    "class DQNwrk(nn.Module):\n",
    "    \n",
    "    def __init__(self,dqn_size):\n",
    "        super(DQNwrk, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(dqn_size[0], dqn_size[1],bias=True)\n",
    "        nn.init.xavier_normal_(self.fc1.weight)\n",
    "#        nn.init.zeros_(self.fc1.bias)\n",
    "        \n",
    "#        self.pr1 = nn.PReLU(init=0.1)\n",
    "        \n",
    "        self.fc2 = nn.Linear(dqn_size[1], dqn_size[2],bias=True)\n",
    "        nn.init.xavier_normal_(self.fc1.weight)\n",
    "#        nn.init.zeros_(self.fc1.bias)\n",
    "        \n",
    "        self.fc3 = nn.Linear(dqn_size[2], dqn_size[3],bias=True)\n",
    "        nn.init.xavier_normal_(self.fc1.weight)\n",
    "#        nn.init.zeros_(self.fc1.bias)\n",
    "        \n",
    "        self.fc4 = nn.Linear(dqn_size[3], dqn_size[4],bias=True)\n",
    "        nn.init.xavier_normal_(self.fc1.weight)\n",
    "#       nn.init.zeros_(self.fc1.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "#        x = F.normalize(x,dim=0)\n",
    "        x = F.leaky_relu(self.fc1(x))\n",
    "#        x = F.instance_norm(x)\n",
    "        x = F.dropout(x,p=0.2)\n",
    "        x = F.leaky_relu(self.fc2(x))\n",
    "        x = F.dropout(x,p=0.2)\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.softmax(self.fc4(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_r = 2e-5\n",
    "d_r = 0.98\n",
    "r_s = 10000\n",
    "b_s = 64\n",
    "EPSILON_DECAY = 0.9965\n",
    "TOTAL_EPISODES = 2000\n",
    "OBSERVATION_STEP = 100\n",
    "MODEL_UPDATE_EPISODE = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn_size=[action_size*state_size,64,64,32,4]\n",
    "net=DQNwrk(dqn_size)\n",
    "agent = DQNAgt(state_size=37, action_size=4, build_network=net, lr=l_r,\n",
    "                dr=d_r, rs=r_s, bs=b_s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Environment initialized.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:37: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 00000300; Episode 000000] reward: 1.0   max Q: 0.280081  ε= 1\n",
      "[Step 00012300; Episode 000040] reward: -1.0   max Q: 0.253853  ε= 0.8691447705514587\n",
      "[Step 00024300; Episode 000080] reward: 0.0   max Q: 0.252982  ε= 0.7554126321769475\n",
      "[Step 00036300; Episode 000120] reward: 0.0   max Q: 0.25516  ε= 0.6565629388651066  Avg. score: 0.09\n",
      "[Step 00048300; Episode 000160] reward: 2.0   max Q: 0.275868  ε= 0.5706482448525043  Avg. score: 0.92\n",
      "[Step 00060300; Episode 000200] reward: 6.0   max Q: 0.253199  ε= 0.4959759378379223  Avg. score: 1.52\n",
      "[Step 00072300; Episode 000240] reward: 5.0   max Q: 0.257834  ε= 0.4310748926911856  Avg. score: 2.22\n",
      "[Step 00084300; Episode 000280] reward: 5.0   max Q: 0.26967  ε= 0.3746664886985753  Avg. score: 2.79\n",
      "[Step 00096300; Episode 000320] reward: 8.0   max Q: 0.272273  ε= 0.32563941935324386  Avg. score: 3.54\n",
      "[Step 00108300; Episode 000360] reward: 5.0   max Q: 0.511458  ε= 0.2830277984162855  Avg. score: 4.38\n",
      "[Step 00120300; Episode 000400] reward: 3.0   max Q: 0.258599  ε= 0.245992130914207  Avg. score: 4.73\n",
      "[Step 00132300; Episode 000440] reward: 6.0   max Q: 0.278352  ε= 0.2138027741808928  Avg. score: 5.09\n",
      "[Step 00144300; Episode 000480] reward: 3.0   max Q: 0.461489  ε= 0.18582556310871748  Avg. score: 4.84\n",
      "[Step 00156300; Episode 000520] reward: 12.0   max Q: 0.534073  ε= 0.1615093164107218  Avg. score: 3.99\n",
      "[Step 00168300; Episode 000560] reward: 6.0   max Q: 0.258417  ε= 0.14037497775371982  Avg. score: 3.52\n",
      "[Step 00180300; Episode 000600] reward: 6.0   max Q: 0.312926  ε= 0.12200617783092295  Avg. score: 3.75\n",
      "[Step 00192300; Episode 000640] reward: 3.0   max Q: 0.348154  ε= 0.106041031436718  Avg. score: 4.08\n",
      "[Step 00204300; Episode 000680] reward: 7.0   max Q: 0.357919  ε= 0.09216500793710629  Avg. score: 5.22\n",
      "[Step 00216300; Episode 000720] reward: 9.0   max Q: 0.401224  ε= 0.08010473467636962  Avg. score: 5.86\n",
      "[Step 00228300; Episode 000760] reward: 1.0   max Q: 0.58608  ε= 0.06962261124037876  Avg. score: 6.57\n",
      "[Step 00240300; Episode 000800] reward: 7.0   max Q: 0.432671  ε= 0.060512128471712424  Avg. score: 6.71\n",
      "[Step 00252300; Episode 000840] reward: 10.0   max Q: 0.907076  ε= 0.0525938000161269  Avg. score: 7.36\n",
      "[Step 00264300; Episode 000880] reward: 10.0   max Q: 0.459357  ε= 0.04571162624744593  Avg. score: 8.02\n",
      "[Step 00276300; Episode 000920] reward: 14.0   max Q: 0.462382  ε= 0.039730020906370435  Avg. score: 7.97\n",
      "[Step 00288300; Episode 000960] reward: 11.0   max Q: 0.469315  ε= 0.034531139904671986  Avg. score: 8.08\n",
      "[Step 00300300; Episode 001000] reward: 7.0   max Q: 0.470101  ε= 0.030012559669326463  Avg. score: 8.14\n",
      "[Step 00312300; Episode 001040] reward: 10.0   max Q: 0.521285  ε= 0.026085259287458716  Avg. score: 8.14\n",
      "[Step 00324300; Episode 001080] reward: 6.0   max Q: 0.484759  ε= 0.02267186669817361  Avg. score: 8.19\n",
      "[Step 00336300; Episode 001120] reward: 9.0   max Q: 0.490924  ε= 0.019705134379357357  Avg. score: 8.35\n",
      "[Step 00348300; Episode 001160] reward: 9.0   max Q: 0.794569  ε= 0.017126614498832217  Avg. score: 8.51\n",
      "[Step 00360300; Episode 001200] reward: 4.0   max Q: 0.491804  ε= 0.014885507428910809  Avg. score: 9.04\n",
      "[Step 00372300; Episode 001240] reward: 6.0   max Q: 0.494734  ε= 0.012937660938842717  Avg. score: 9.52\n",
      "[Step 00384300; Episode 001280] reward: 5.0   max Q: 0.507325  ε= 0.011244700348163017  Avg. score: 9.65\n",
      "[Step 00396300; Episode 001320] reward: 7.0   max Q: 0.489351  ε= 0.009773272504024051  Avg. score: 9.6\n",
      "[Step 00408300; Episode 001360] reward: 7.0   max Q: 0.507645  ε= 0.008494388688046865  Avg. score: 9.28\n",
      "[Step 00420300; Episode 001400] reward: 13.0   max Q: 0.500109  ε= 0.007382853507247396  Avg. score: 9.06\n",
      "[Step 00432300; Episode 001440] reward: 11.0   max Q: 0.628953  ε= 0.006416768517571567  Avg. score: 8.99\n",
      "[Step 00444300; Episode 001480] reward: 7.0   max Q: 0.509318  ε= 0.005577100800886566  Avg. score: 9.17\n",
      "[Step 00456300; Episode 001520] reward: 7.0   max Q: 0.531026  ε= 0.004847307995928911  Avg. score: 9.36\n",
      "[Step 00468300; Episode 001560] reward: 10.0   max Q: 0.529052  ε= 0.004213012395913888  Avg. score: 9.15\n",
      "[Step 00480300; Episode 001600] reward: 5.0   max Q: 0.534201  ε= 0.0036617176921770263  Avg. score: 9.22\n",
      "[Step 00492300; Episode 001640] reward: 7.0   max Q: 0.83148  ε= 0.003182562783391418  Avg. score: 9.5\n",
      "[Step 00504300; Episode 001680] reward: 12.0   max Q: 0.505147  ε= 0.0027661078001363464  Avg. score: 9.56\n",
      "[Step 00516300; Episode 001720] reward: 11.0   max Q: 0.525473  ε= 0.002404148129270105  Avg. score: 9.79\n",
      "[Step 00528300; Episode 001760] reward: 8.0   max Q: 0.525656  ε= 0.0020895527741861845  Avg. score: 9.88\n",
      "[Step 00540300; Episode 001800] reward: 12.0   max Q: 0.378357  ε= 0.0018161238664752158  Avg. score: 9.73\n",
      "[Step 00552300; Episode 001840] reward: 11.0   max Q: 0.517473  ε= 0.00157847456122063  Avg. score: 9.64\n",
      "[Step 00564300; Episode 001880] reward: 9.0   max Q: 0.538903  ε= 0.0013719229103334198  Avg. score: 9.66\n",
      "[Step 00576300; Episode 001920] reward: 11.0   max Q: 0.664785  ε= 0.00119239962311603  Avg. score: 10.37\n",
      "[Step 00588300; Episode 001960] reward: 9.0   max Q: 0.51548  ε= 0.0010363678968388277  Avg. score: 10.4\n"
     ]
    }
   ],
   "source": [
    "#env = UnityEnvironment(file_name=\"/data/Banana_Linux_NoVis/Banana.x86_64\")\n",
    "#brain_name = env.brain_names[0]\n",
    "#brain = env.brains[brain_name]  # Refer to the code \"1. Start the Environment\" above.\n",
    "print('[INFO] Environment initialized.')\n",
    "\n",
    "scores = []\n",
    "steps = 0\n",
    "epsilon = 1\n",
    "avg_reward = collections.deque(maxlen=100)\n",
    "state_data = np.empty([37*4])\n",
    "for e in range(TOTAL_EPISODES):\n",
    "    env_info = env.reset(train_mode=True)[brain_name]\n",
    "    state = env_info.vector_observations[0]\n",
    "    if e is 0:\n",
    "        state_data = np.stack([state, state, state, state]).flatten()\n",
    "    score = 0\n",
    "    while True:\n",
    "        if np.random.rand() <= epsilon:\n",
    "            action = np.random.randint(0, 4)\n",
    "        else:\n",
    "            action = agent.get_action(state_data)\n",
    "        env_info = env.step(int(action))[brain_name]\n",
    "        next_state = env_info.vector_observations[0]\n",
    "        next_state = np.append(next_state, state_data[0:-37])\n",
    "        reward = env_info.rewards[0]\n",
    "        done = env_info.local_done[0]\n",
    "        \n",
    "        agent.append_replay_memory(state_data, action, reward, next_state, done)\n",
    "        \n",
    "        if steps > OBSERVATION_STEP:\n",
    "            agent.train()\n",
    "\n",
    "        score += reward\n",
    "        state_data = next_state\n",
    "        steps += 1\n",
    "        \n",
    "        if done:\n",
    "            scores.append(score)\n",
    "            avg_reward.append(score)\n",
    "            break\n",
    "             \n",
    "    if e % 40 == 0:\n",
    "        max_q = agent.max_q(state_data)\n",
    "        if not e >= 100:\n",
    "            print('[Step ' + str(steps).zfill(8) + '; Episode ' + str(e).zfill(6) + '] reward: ' + str(score),\n",
    "                  \"  max Q: \" + str(max_q) +\n",
    "                  \"  ε= \" + str(epsilon))\n",
    "        else:\n",
    "            print('[Step ' + str(steps).zfill(8) + '; Episode ' + str(e).zfill(6) + '] reward: ' + str(score),\n",
    "                  \"  max Q: \" + str(max_q) +\n",
    "                  \"  ε= \" + str(epsilon) + \n",
    "                 \"  Avg. score: \" + str(np.average(avg_reward)))\n",
    "    \n",
    "    if e % MODEL_UPDATE_EPISODE == 0:\n",
    "        agent.update_target_network()\n",
    "    \n",
    "    if len(avg_reward) is 100:\n",
    "        avg = np.average(avg_reward)\n",
    "        if avg > 13:\n",
    "            print(\"[INFO] Training completed. Total episode: \" + str(e))\n",
    "            break\n",
    "    \n",
    "    epsilon *= EPSILON_DECAY\n",
    "\n",
    "with open('save\\\\scores.pickle', 'wb') as f:\n",
    "    pickle.dump(scores, f, pickle.HIGHEST_PROTOCOL)\n",
    "    f.close()\n",
    "\n",
    "agent.save_model('save\\\\model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAEKCAYAAAASByJ7AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAIABJREFUeJztnXecFGXSx3+1ibBkdslhyTkjSBDBiC6IGDDcGVH0zHd6nr7iiXcG1DOcEXNARfTMggpiIEhwyTktS1xYMgtL2PC8f0z3TM9Mx5lOs1NfP7gz3U/3U/10T1U/9Tz1FAkhwDAMwzApXgvAMAzD+AM2CAzDMAwANggMwzCMBBsEhmEYBgAbBIZhGEaCDQLDMAwDgA0CwzAMI8EGgWEYhgHABoFhGIaRSPNaACtkZWWJnJwcr8VgGIZJKBYvXrxPCJFtVC6hDEJOTg7y8vK8FoNhGCahIKKtZsqxy4hhGIYBwAaBYRiGkWCDwDAMwwBgg8AwDMNIsEFgGIZhALBBYBiGYSTYIDAMwzAA2CAwDBMDv2/eh/y9R70Wg7GZhApMYxjGH1z95kIAQMHEXI8lYeyEewgMwzAMADYIDMMwjAQbBIZhGAYAGwSGYRhGgg0CwzAMA4ANAsMwDCPBBoFhGIYBwAaBYeJi+4ESzFq7x7X6KioEpizahtLyCtfqtItf1hVh2/4SCCHwyaJtOFlW7lrdczfuw6aiYtfqU2NB/n6s230kbNtPa/Zgx8ESjySKhgPTGCYOhr8wG8dOlbsWoPW/xTvw4BcrceDYKdw+rK0rddrFDe/9gRQCXryqFx74YiW2HSjB/cM7ulL3n9/2PpDuyjcWRMlw0wd5qFk1DSsnnO+VWGFwD4Fh4uDYKffecgHg0PFTAICDx065Wq9dVAjgyPEyAMCBBL0Guyk+Uea1CEHYIDAM4wlEXkvARMIGgWEYhgHABoFhGJcREF6LwGjABoFhGI9gn5HfYIPAMAlIIvvfBXcQfAsbBIZhPCGRjVplhQ0Cw7hIRYXA5AVbVYOy9PbFU9+HC7biRKn5c27dfwwz1zgXbJfsHYTvVuxC4eHjXouhChsEhnGRL5fuxMNfrcIrv2yO2vf18sC+l3/epHm8VXfLtJWFGP/VKrzw00bTx5z97G+4+YM8axXFQLJ2EO74eCnGvD7fazFUYYPAMC5SfKIUAHCoJDooSw5QOlRSalt9R0/K5zQfBFZW4fA7PA8ioPDQCa9FUIUNAsO4CLnsOGfdy1iBDQLDJAE8gMuYgQ0Cw/gEJ97m/RwExkbKf7BBYBgP0FP+ZhSlddcTa1/GGMcNAhG9Q0RFRLRKsW0CEe0komXSvwudloNh/IAZPe6M398/PQX/SMJE4kYP4T0Aw1W2Py+E6Cn9m+6CHAzD+AjiXovvcDxBjhBiNhHlOF0Pw/gRIQKBYZf2aYbqGaGfm55vX6sXcbKsHO//XgAAOFxSik8WbcOV/VrguZkb0LBWFQDAyB5NUKtqOgCgtLwiWF7pMlq/uxi7Dh/HsA4NYr4uMyzeegBCBHIgpKaE6p+yaLvBcQdRIQROy6nnqHxqrNp5GEXFJ7D9wHFcc3pLpKToG62KCoF35m1BeYXATWe0DrtOADhVVoGPF25FkzrVdM/z3Ypd6NGsDprXqx73NcSDlxnT7iCiawHkAbhXCHFQrRARjQMwDgBatGjhongMEz+/rC/Cw1+vxvo9xXjs4m6m3om1XEYvzdqEXYcD89en5m3H1Lzt6NmiDl6cFQo6W5B/AC9d1QsA8M7cLdiw52jUec5/YTYA57OHXfqaevDV2sJAGkktw3fpa78D8Ca72YiX5gY/183MwEU9muiW/37Vbjw2bS0AoGbVdFzdP1xHvTknH8/8uF71WKG40Xd8vBT1MjOw5OFzYxXdFrwaVH4NQBsAPQEUAnhWq6AQ4g0hRF8hRN/s7Gy35GMYWyiRMqodPBZ/sJmcLU3JqbLw3MoHjp0Mfj583L4At2Sk5KRxJrNjp0JljqmUt3IP/JBBzhODIITYI4QoF0JUAHgTQD8v5GAYt4h0EcUyy0jtmEg/vLKMsrgfp3j6UCTbETo32o9Bg54YBCJqrPg6GsAqrbIMk8jICjv449fRzHrKA1CfnaOn6P2ocCodijZWuxeJdg8cH0MgoikAhgLIIqIdAB4BMJSIeiLQnAUAbnFaDoZJBsJ7CM5oIyPDZRa3l/HwAr2W8qOtcGOW0VUqm992ul6G8QOyzjOjQ40UpGU97JDGSbS3XsY8HKnMMA6ipeLVdKrxm3f0/pQII+LGUhVsD0LE09529bTshA0Cw7iArDjicZKoDirrjSEoy8VRb7Qc/lNkTmD1KtV6eInWVGwQmErD2sIjmLVWP9PXsZNleG/eFl2ltv1ACb5autNy/fM27cPSbeHhNFouo2+W7cL2AyURZQOFP8vbEbb988U7NDNsbdhTHPZdiEDugw8XbEWFSl6DsvLwaaqbiorx1px8fL0s+nq/XBqQ49M/tqPoSGj9fuVZl20/BCDQZs/N3IDXf9uM0og6nOBEaTnenhsICNPib1OX4YdVu3XPU1ZegZd/3hh1L4wQQuDadxYhryA6fOqTRduw7+hJlaMizqGybffhE3hrTn7UdGK38DIwjWFs5YL/zgGgH9D02LQ1mLJoO1rWz8SwjuqRupe89jv2Fp/EqJ5NLA18/umthSr1S7OMIsoePVmG0a/OQ974UCCSbKSOl5ajqPgEGtSsiqMny3DvZ8vROitTNXL37k+WRW3726fL8fO6IgxsUz9q38eLtoV9P+e52cHPo3o2Ddv316nLMaB1Fu7/fAW6Na2Nb+8cLMkZKnPxK/NQMDEXE79fh2krC4PbbzmzTVTddvLfWRvx2q+bUadaOi7t0yxq//YDJfhi6U58sXSn7vOwbncx/jNjA1buPIzXr+lruv5Za4swe8PeqO3b9pfggS9Wot+Snfj01gG651B7Jzn9yVkAAkmKbnW4DdXgHgKTVMjBP3o5hvcWB97u9N4+zRJpT5Tf9x3VDkSS666QtEZR8UlT/moBYL90jScVb5lyvXJWNrPIb/vKN141OXYpejBHTQR0xcsRKeCrROM+njT5hi238y6LGcyOa9R7Sm4vKUAw1jEGr4IK2SAwSYnei7+8HI2dqSSt+pLl8hT8LkyfQz6mwiEHttppldvM9qnimXVq15XJ51Fbsshq84WdwsSxesZCzd3nBmwQmKRE78cuL1BmxSBojUm4PtNeGYdgdRBa5YBIw2SiWlfRCfMzdXzQaFq0TlrFI7fHapPt6J3GAhsEholAnspZXm7nj9LauVSnpZo8NjiQrbZPR4Wq6SC1t1hVJRfWRTCnXONZ/tpI0ZpVxGYNnlXMVK8nY7lH05PYIDBJiZ7OknsIpRXxz/SQB6VDisec6pHf1oPHw5ySE0ZdBB3UXEwVwRfokNxGfnE3e0Wa6z6ZPoPczvbIonTxxQO7jBjGJ6TKPQRLLiP17W67jDQXtzMhidkxB9UxBFNHhmOHEo73RdrOF3FCdCxCrIaBewgM4xNSYhhDMMKUC0H52aT7Rvd8lscQ1LapuIwsSeENZvVpsAekdo54opBjPjKARx0ENgiMO5woLccbszebeuv+dvkubCoqNiynxGwwlKwobv1wCV77dTPmb94ftn/G6t3BKX/Lth3CuA/yooK/1Fi7+0hEPQJvz90SnII5Z+NevDk7P2q64md527HjoHZQlKyoSk6Vo8zEmEbe1lCg1Mqdh4Ofv1cJ0IqcR//j6t24/eMlYduUt6vkVBmGvzAb2/aHyxs5A2rf0ZN4cvpaQ1nX7w6166qdhzHug7ywtpi+shBPTl8brG/2hr1YvPVA2DmUBm7jnmJ8t2JXVD3KoLPPFweC7e74eAlemrURY14PJPGJXAIEAB79Zg2EEHj/9wLTuQq+lAIat+4vwZhJ8zFtpXZgnJ7R+nHVbqzedVi7gENwYBrjCi/O2ohXf92MOtUzMKZvc92yd05ZCsBaxqwpEQFXZnjqh3VR9YybvDj4WVaOM9bsMZTlr1PDA8TmbtqHf3+3BnWqy+ksBR6fvhbV0lPDyv39fyvQuHZVzH/wbNXzKvXUN8ujlZ0aS7cditqmFjl77TuLwr6rBbkp33WfnL4O63YX48IX54SVmL95f9jb9EcLzd2LuZv2BT/LmcrWFIYM620fBdr/07ztWPrP84Lyat2Lc58PBNmN6N4kTJ7LJv0e/HzvZ8vRr1U9fLeiEN8hFEh3oiw6ruBUeQVW7zqCR75ZjV/WF+G9G/TTthBRWPa6RQUHdErrs//YKeS+ONf1rHHcQ2Bc4ciJwFu3XkBYPLgRDKXHpqLwVJXHpUxpkYFgagFNciCcEr+sgaOU42CJ+lvySRuXqtivEqx3sEQtSMt8A+05Et6+ar1Jrd6XHGh2SFWGCIks3jQ3FiK0ChsEhrGByJ+2XsCT0bGBbc4oC6vjuLIURDpGSnhnwLQGy/XkUdtlRxCfVb+/X4y+EjYITKXA9HROh+UI1iP92tV805GoKSN5k9dKw42EO7EQT7uovcnbEQjmn9aJHTYIDGMDkTpG1i9mDILXSl8PoZinr/vWbdM12JVETU8eNd1vi0Gw7DLyH2wQmEqB2bdXt+ICRNAgxHi8faKEYVXhhvUQLLphnCQew6HWI9Oa929Fx/vZsJuFDQLjCk4tEWAV11xGMO8yCh4TpnyFdB7r6Bkhq8tFKGXS8rMLCNeT5hguXaG7cJy5bUqiVq1VaUfrcSL+syBsEBjGAUJLPrhftxUjZIRsBAjkSrCUVcnVLtVo2Qc1xa09qBx/tHoiwXEIlZSDx07h8yU7MHZwK0tJXiLZW3wS363YhesH5sR1HjOs2nkY2yxmrpJZrJK56vPFO1A1PRUCAiO6N4lXvCC/b94HIYD8vUdV9781Jx/pqYF3rRSTPqMJ36zG96tC8+JX7TqCf323Bv8Y3tGyfFoR1h8u2BqVIMeIpdtDMQ0/aWSj+2ltEdbtthZIKDP8hdloVrda8PuxU+rTkpUZ475ethOf/LEdAPDgFytx+HgpBrQOJQOaNHszhrTL1qwz98W5Kuc/gf0qsRo3vpcHIBDbsXFPMb5dUYjqGamoXS09quyT36/TrDOSsgqB+z5bbljuuZkbUDU9BbcNbWv63PHABqGScv/nKzBzzR70alEXfVrWjfk8d05ZggX5BzCobRbaN6wZv2A6RkUOToqFWeuKorbdq/jB2WkQrn5zoe7+x6aFonTTUsx1wt/7vSDs+11ScN4fW2IPbopk/FerLB/zsIljPjYZiKbGut3FpoyJMq1oZADdxAhF/PQP65EWw+DN3z6NVtDKRDUPfbkqGGyWkRq/c+XH1frpXgEEA93+fHpL1KoabYTshl1GlRT5QY43v60ckONGnlw3cLtbH+ugsozWG7PbeOH6UnJSJZJYj+OnrD+vRlnKtiuW1Tjl8u9BuFQdGwRGF6fdRJUdbj57sDp+EctqoX4eAnAq+10kbBAYU1SGATMmdry2a1YVolf5BJzCrathg8Do4rUiYBgAljViTD0Eg2O8/C24NUWVDQLjCv55X3NXEjungCYzbvQQSm1NmWov3ENgKiXJph6T7XqdwuoLcixLUZQbRKd5OZ7mlsuWDQKTZLCKTkSs6vdYhhDMJCDyCrcWFmSDkMQszN+PWWv3QAiBN2ZvVk2i4sZiY1r8sr4I3y7fhaXbDuKlWRuxZNtBnPnML1ix4xC+WLJD87ib3v8japriln3H0OPRGZrBZGZ4b94W5EYkh0kWCvbHFjBoF+/M2+JoeQDI33dMd//OQ8d19zvJsz9ucGWgnAPTkpgr3lgAAPjuzsF4Yvo6zNm4D5PH9lct68Usoxve/SPs+7MzNwAALnp5HgDgkt7NVI/7aW0R3o8I9Hr029U4fLxUda75niMn0LBWVUN5Jny7xozYDGM7U/O2446z2qJ5veqO1sM9BCYYdHbkRHTWsUQdEz1RGu4P3n34hEeSMIw9uBGLwAaBMeWd9FNyFLtx0uhV3lZj3MaNXjobBEYXq8slJyJOXiMH9DF2wT0ExhXMqMP4n8VQ5i03iKxGP7mLcz+0ytyzYtzFjSdJc1CZiJbqySCE6G2mAiJ6B8AIAEVCiK7StnoApgLIAVAAYIwQInr9YsYVkkFl6SlmJ1+8uIfA2IUb0cp6PYTLAFwOYBaAXwGMlf79DOBrC3W8B2B4xLYHAMwSQrSTzv+AhfMxjCGRPRH9HLtO9hAYxh7ceLnQ7CEIITYDABENFEIMUuxaSkTzADxqpgIhxGwiyonYPArAUOnz+wgYnH+YkphxFbtdPF6NSej9luxIsK5ZL1sExibceJTMjCHUIKLT5S9E1B9AjTjrbSiEKAQA6W+DOM+XNMzZuBe/SMlgvl9ZiD8K9BOo3DJ5MbZIATcny8rx+LQ1we+RKFW1HKy250hguqbWw7h460FMW1EYdZxymueT09diyqLtYccV7DuGB79Yic/ywrdbQQiBx6etwZhJ86P2/WfGhrDvm4q0A9J+WLXbsK7HvostBkEt2I9hYsEvg8pjAbxFRJuIaCOAtwDc5KxYIYhoHBHlEVHe3r173arWt1zz9iLc8F4gYOsvHy3B5SrKUMnh46UY/WogkGv59sN4c84WPPOjcaq//H3H8MT0ddhzRF+hXfra77j94yXB71v3l+CJ6eswbnJecNvrs/Ojjrts0nxMWbQNf//fCkNZtMjbehBvztkSzGIVK8oMZ1q8Ndd65CvD2ImnLiMAIKJUAC2FEF2JqH5AKLHfhnr3EFFjIUQhETUGEJ3/UEII8QaANwCgb9++3AGPgWIp4Ex2jew7esrwmEg3itkBLTmf71GVIDclh48by2BEZcnixjBm8LyHIIQoB3CP9Hm/TcYAAL4BcJ30+TpYG6RmLOKu1164Vym/HjBJhF8C034konuIqDER1ZL/ma2AiKYAmA+gAxHtIKKxACYCOFdyQZ0rfWc8wokHTcse2DlIzfaASSY8dxlJ3CL9vVexTQBoYaYCIcRVGrvONnM8Ez9mlbCynJu9CiFETGvNu5VnlmH8gBtBjoYGQQjR3HEpGF+gH80b/zm0KKsQSE+NxSBYr4thEhU3nndTy18TUUcAnQEE1wgWQnzslFCMvchz/+Nx11hV9EZv/MrzlVcIpKfGIhNbBCZ5cON5NzQIRDQewHkAOgL4EcD5AOYCYINQybDDvx/LI1taXoGqMVgENgdMMuGXwLQrAAwDUCiEuAZAD3BiHUf5cukOLN0WvrTT54t3YNTLc00dP3l+ARZtCc3NP1VegYoKgZs/CMUGFJ8IJYpZU3gEgFEvQGBh/v6wIDQlC/P347sVu4LftWzLS7M2Rr3pXPDfOXhuxnq9ylWJTKATDyWnyjDyJXPtyzBe4IseAoDjQohyIiojopoAdgNo7bBcSc1fpy4HABRMzA1uu/ez5aaPf/jr1VHb9hSfCMYjAMAzP4YU8MNfrTJ1XjnDWm73XM19M/46RPccuw6fwJIIY7fj4HG8+PMmUzI4xV1TlmHlzsOeysAwevhl2ulSIqoD4B0AeQAWAViifwjjNyIHpEpOlUeV0XMZmX0YE9WtH5mDmUkcLuzWyGsRYmbNv843XdYXg8pCCHna6StE9COAWkIINgiJjIkHK97xBL3jU3yYlzM1xX8yMeaIZcqyX7DyW/CFy0jKZzAHwBwhhLf9eiZmIh8mp56tyLnSag9xWor/8jKlJrBSSXb8+IJhFiuyu9FDMPPL/ARAKwBvSgvcTSWi2x2Wi7EZpV4W0n9O1iNPdVUzPH58G/ejTIw5EvnWWZHdL4FpM4joJwC9EYguvh1AHwCvOCwb4xBCwPIcNquPot6LT2oK+W7KKBuExCWRe3fWXEYOCiJhxmX0I4DaAP5AwHV0uhBil/5RjJ+JwR6YP3fEidXqSU0h3wWVJbBOYRL43ll57vwyy2gDgDIA7QC0B9CWiKo4KhXjO2J9GNUUvx/fxn1mnxgLJPIYgpUBcTdcRoYGQQhxpxBiMAIBaocBTAZwyGnBkpHCw8fx4qyNwe/PzViPPUdO4PmZG6LKzlq7J/j5vs+W49FvV+u+df9LkfFr8daD+Ha5eifvVFkFnvphHY6d1J6G+dCXK3WvAwDW7S7GoZJTePTb6ExjQgjfrUP0vYmsaYw/SWSXkRV8MahMRLcS0UcIuIwuA/ABAjmRGZu57aMleE6h/F/8eRNOf3IW/qswEjJj3w9FHf9v8Q68O68Aq3cd0Tz3zDV7wr6XqTxdBGBq3na89utmvPRzdJ0yHy3chqLiE6r7lG8xj3yzGpMXbFUpw7jJ3We381oER3HKHvRqUSfmY+8f3gEA0L9VPbvEQbVYFvyyiBmXUV0ArwLoKoQ4UwjxsBBihsNyJSUlKm/lVlwZdiSLLy0LZCE7WaafjcyMXMdVgt/cpnVWptci2Eq9zIzg55pVjBcaGNI+Gz2bx67Y7OKec5wxSk3rVDM0CEPaZwMALunVFDMVkfS53RvrHvflbYNMy7Hsn+ciu2bIk96reV0UTMzF1FsGmD6HHq2zMtHPRuOihRmX0ZMAygFcCQBEVI+ITOVCYNzFyfwAZuMYlNu17JOb/vrKnDPBzJUlhzPFPE71Jiiipe2up9yl59jsaqeDALRBwF1UDYGVTgc7K1ryEe+gkYDfloTWksU9Gf02VuE2fnGvRypMu8+uR/hvwjGL4Ch29P7NYMZldBmACwEcAwAhxE4AplNoMuaJV5cL4dyDE8tZ/dBDcOuH5AVmjD8F/1c5IbJm9MLK2vhoRMpgd5NX+MggnBQiFMpERNWdFSl5if+Wuzd7x0xvxg+9lbIK/bGQRMbMvfbLOj9OimF0auVj6JQYkee1u93dchmZMQhfENErAGoT0Q0AZgB411mxkpN4FagQzvnMYzmt9w6jwDTayooZo0yo1B0EUyjbKUxR29gwRBRueOweQ3DpMTazdMVTRHQBgFMIJMd5XAjxveOSJSHxKsoKEZ87xlKQTIIMKpeWe99LsROreibgUqm8JsGyy0j5xU6XkX2nUsWtyRGmMp9JBuB7AKAAVwghpjoqWTIS5z1/4PMVuOfc9rFXr/PQme29jFRkdZu9Ya9qmevfXWRNsDg45darlQeYuyX+MAZOSVEtPdX0gLWAg7OMIgyT3dV4PqhMRDWI6O9E9AIRnSUZglsBbAZwrSvSJRnx3vL8fcdw15Sl9sgSIUze1oPqBWOg8LB6UJsT9G1Z17W63CbyeenVog7O7dwwbJuXnQN53vzV/Vvgyn6xzVRv16CG7j0cn9s57Pvfz+8QVebpy3qgd4s6uPvsdmHLXOi9dcvnye2mH6sgQyC8cnXv0HeVdq+hiBt58pJuuG5ASwCBWAoAqFlV+/3cD4PKkxFwEW1EYIXT6QD+DGCMECI6hyITN14Pwuq5FiLfUBLFEeNGdKcbnN5aPyipUa2q+PK2QahbPT1su/KOntEuK2xfVg3jJckGtK5vWsZIPr1lAAom5uKJ0d2QXbMKMjPU78WSh8/VPMcXtw1EneoZqvtqV0vHkPbZQeU7YWRnXNyraVS5pnWq4YvbBiEnKzO4jlbtaumq0foAcHHPJrh9WFsAwCt/6h2WylYLooAB7NRYnoAZ/VuSFT8AXNWvBR4d1RUAMO+Bs1AwMRcrJ2hnT/NDHEJbIUQ3ACCiSQD2AWgphNBeH4GJCz8p2cgBy0ReQKwyEOs8fiJt94Wpaasu3HajtQ61ZJC3y7sFjF01aamBEuUVQvOtO55hJ7l+NZnjaUu3xhD0egil8gchRDmALWwMnMUHszSDRMoS+TB73ZthzMYhhG5cZHG33jqN0DN2RNp7SVEmVF6/LjlbX3mF0OwhxOKeCRonhwyoW7On9XoIPYjogPSZANSUvhMAIYRwfmENxjOMdIVPdEnSoKZoIrPgaR2npaTc8kvHA8F879TMMym7jMorhOZAbSyxK1FLV1g+gz5+cBmpO+4Yx3BjvfNkozK3qPLatPSFUpdG57u2X6aY0NGeerYgcswr4DLSV8XpsstIaBuEWCamRUUq2x2Y5pLx1jQIkpuIcRGvf6BKF0Sk8uARBP8Rfr/UIek/Ncy8dfp56CjSX2/GhWamhxCLvz4qUlmtjJ8bU8JMpDLjEp4bBOXnCFlSfJjlLBkJf+NXfNa2CJpl/LLOk24vAGRolJQGz+wYAqDtGtIaW7BCAuh+Vdgg2ERZeQWuemMBnvlxXXBbRYXAk9+vRdGRwLz7TUXFOOPpnzF7w16MfnUeXpq1Ea/+ugk5D0zDCz9FZ0XzkoVbDoR9f+bH9WHfD5WU4qz//OqiREwk4QpeYwwB2srJ6xcQM+i7jFS2GZxPmb5VazZRPGMriWoIZNgg2MQv6/difv5+vPLL5uC2hVsO4PXf8nHf/1YAAC6bNB/bDxzHte8swtJth/DszA14+oeAon3hp42ez9yxUv3j09cgf98x54TxCVpz593GSNE8dnE3AMBdEdnR/n5+B/RpWRdntMvChIu6BLfndm+Md284TfecE0aGB321b1jDgsTmMdKhVdL01VTIZWSuvot7NsH7N/bDk6O7hW1v2yBwfY9EXLeSl6/uhZE9mgS/T/pzb+R2axyVJ1zNTff46K6Gsj2quEcyw7s0wnsG98ou9CKVDxLRAZV/BxWzjxgJte63rOBPlQWGY06WGmQhs18sS1ip3yijmhec06mhcSEVBrXVDr6qZtEgrPv3cNNlX7yqV5QCl4leTllfbXZsVBMA0KxuaDHizo1roWX9TFRNT8Xksf3RvmHN4L5Xru4dFnSmzPYFAM9c1h3XD2oVVu/UceHZv16+upeuTJFoerUMrJ32flL8P2yTLi9c2Qtnts9G5ybhq/hf0rspCibmop2inSIZ1qEBXrqqF6pLz8Wgtll45U+9TY0PyPdIj+sG5iAjNaSWx+d2wqRr+mBohwaGx9qB3iyjLJ19jBki5+4bqNxE6MLL+FFWtR6W170uPfSGZayKHctVWpnBo1rGhakGVlxGAsJxmeKZTeRGe8WL6VlGRFQPQFXFpl1OCVXZMPvj9nzaqQUt5IWiJdIXMVaJ9H6oTl6mnQoilvvhl1kvelKYaSOrLiNATb47AAAfcklEQVQtzBxvFG8g71f7LfukuXUxHEMgolwi2gBgB4CF0t+fnRYs0VAf4IqYJ12Jgr28mKBi9HtywkhZPaOVH72ViVtG51WT0+7W8Ft+6shIZSdXMw3WaXKZjbiaykPDYWb568cRyKk8QwjRi4jOBXCpHZUTUQGAYgDlAMqEEH3tOK/f8NfPSBsrcnqhHMigixCrUvTqzS22/BPqx7hxO5yqwupMoqgy9oniKInQQzBjEMqEEHuJKIWISAgxk4get1GGYUKIfTaezzcEHwAR9kcTrw2HFaXixRx2o9+TEyI56Rqzs4dg99Ojtkhb5EuAGwpO150Utb6W88YhEZR6PJgxCIeJKBPAXAAfEFERAP9NMfEhIXtg7sfq5wHQSLzoIQTWtIk/iY+TWBkXsNOHr3bptreHQ81rtLid1nUEj5PdNBCOj4tEuYE1yqltN/tsKEu5/UibMQgXAzgB4B4EEuPUBjDCpvoFgBlEJAC8LoR4w6bzusb0lYXYdeg4Ppi/NWrfD6t3Awjc1JJTZYb5fY+cKHNERrOs3HkYK3ceNlXWkyBXIz+6Ez0E+08ZxM7gbzduh/fmVh0vZu/oT4TV2JcAvQszgWkPCiHKhRClQoi3hRDPAfibTfUPEkL0BnABgNuJaEhkASIaR0R5RJS3d696SkYvue2jJXhs2lpsO1ASte/deQXBz2oGI5JESghvVw8hMqBHD72iD4/QDiYyYkJEMFDV9BQ8MrIzxud2wtvXRQcE3XxGK015jH70SjnjzTFxZvvs4OfWWZnBz/cPj84aJvPE6G6410Sa1fO7NgIQCJTKqpGBczs3RHaNKrh9WJtgmaEdsrUOV8XMAnwA0EURH6DWQlf0bY5aVdMw6Zo+UedXlr+kd1O8HlFGiyHts3G1iaxusqzv3dgPo3o2sRS4aPZuTx7bP6o+tzBjENQibWzJmCaE2CX9LQLwJYB+KmXeEEL0FUL0zc629gD6BYHEmkFkBruup7qFjGZ6b4JjB7cKrrdklPbw3etPC0aEXnN6S7TJDo/ArZ6RhhsGtcJNZ7RGH5X0jQ/ldkbBxFzkP2n9ZxB2PktjCNGFqyuUUZoimGloe+0gpqv7t8CdGsFwSuR0j62zayBv/Ll489q+SEkh/P38jiiYmIuCibmonpEWlgXMLv57Zc/gZzWF+NRl3bFiwvno2bxOVBnl5+fG9MT5XRqZqvODG/uhbqbxAs/y6U/LqYf/XtnLWhyCybL9WtXD2MGBlw639YZepPItRLQUQAciWqL4txHAmngrJqJMIqopfwZwHoBV8Z7Xr6RWskVCvBlD0N8vrWwcJpuamEYL9cXje7cyNTbZstCZvVyrzR8cq1Mc6FTTxjNGYeVIr54MvTGETwHMAvAkgAcU24ulN/p4aQjgS6mB0wB8LIT4wYbz+g4hBFJTKpdF8GzaqQ6ygjWaAZWWYryCplMoRYt3DCHRep2xyGsqYlrZQ5BUaZpDq/OaPavaS0Ui2H+9SOWDAA4CuJyIugIYLO2aAyBugyCEyAfQI97zJAICobfXyoJbKf2UGDWh/OZvNOCtfDO3OzrcWIHF1kOw0mtJBMWjJHoJithQNpGVsSkrGLatTgG/RIbrYSZS+XYEegstpH+fEtFtTgtWmRDCuQfUKzyZ4mnQhHITG8mWmuLdqjJK0WKRIQF0imWM7obRkxZaLgLBRk11qKHcVupuL2djZtrpLQD6CSGOAgARPQHgdwCvOilYZaPyuYzcr9OwhyC7jEwYBBm37ZqyuniVix8WS3RkuRCrYwiK5SJkebx+AYu3Vbwy/Ga0FAEoVXwvReJEi/sCAR5UtgOjwWA1l5GalKkppPuLc3IZGmWzOaWzEq0XEa+8ysPlbGdpHv3gEqzpo9DsIRBRmhCiDMBkAAuI6HNp12gA77shnFd8uGArWtSrjiHt1ae5zt6wF1sPlOD8zurr709bUYh/fh2aMLV8+yEs337IEVm9oqj4pOt1mu0hGGW8csqdYIawWUZxWgQ/DCo7IUKsbhIBEbz3nvcQfHBvYkHPZbQIQG8hxNNE9AuAMxD4Td4qhPjDFek8YvxXAWVeMFF9nvm17ywCAPy2Xn1s/faPlzgjWCXk/bH9cMmrvwMAbj2zDSb9tlmzrNYg7Mc3BQJ5HrygI0rLKjCwbX3M3aS+PNaonk3QqXFNtMrOxOwNe3G3NCf/oQs74fHpaw3lvbxPM939aiL2aFYby3cEIsDDXEYAbhiYgxdnbcT1A3MwvGsjfLs8sKr8yB5NcLKsAnd8vATFGhHsD4/ojI6Na6FDREKXttk1MKpnE/xlaBvV49R4+7q+eO/3AnRuXMtSUqA3r+2LES/NNV1eDUIgg9u0FYU4t3NDtM2ugSk3n47vVgTa4h/DO+JEaTnqZWZgeNfoGJMbBrXCmsJiXDcgB7WrpePS3s1w3cCWcckUK3a/a/hp6YrgpUkGoFIbgVgo1UrK6gIdGtbE+j3FntVvF71bhAK1crs11jUIWj+2gW0DuZya1KmGSdf0wad52zXP8d8rA1m+aqSm4M1rQ4vr3jykddAg6P0Ix8cQEf31HYMx5vX5WLTlAIQAujerjRU7DoOIUDczI+zF4/TW4dnb/jmiM/4upWCNpHm96vibStRxWmpK8DrN0r1ZHTw3pqdxwQi6Nq2tun1Uzyaq27V45ereeOXq0PcBbepjQJtAWzSpUw2vX6O9EHLdzAy8dV1o/7NjkmLyoiPoGYRsItJcokJawoJhbKPMcC6r9dcvuwc9jTwRWgPFyoUOZZHMeDUSYaqiWbRcQZXpGhMdPYOQCqAGEn+cxDES1E3oW4wCyvygN2JVXsql0GXFaC0OIaZqPcMHtyqh8cpI6hmEQiHEv1yThEl6ygwMgh9COWKVQVb+ynWt/GDg/EDlbIYEs+ASenOzKud9shEv19+vjMrEaHaQH5KUxypD+Fz5wOdkW8soGUj0O6pnEM52TQqGgX96CHqGPlYdrky+LsdwxHKuRFc4arBd9A+aBkEIccBNQRKRRPPr+h3jMYTE1Rxqydf90ONxCieWhU5G3FYxlSx+VhshBB6ftgabio4CAIpPlOLBL1bg6MnwOd77j0YHXBUVn0DOA9Pw7rwtmPDN6uB2rbnuTGx4kafZbWIdQ3B7TRsmPhL1ZTFpDMLOQ8fx5pwtuP7dQFDZm3O2YMqi7Xhn7pawcmrBST+sCqTCfPTbNXjv9wLHZTWidVYmqlhILOM3tOaon9E+K+x766xM9G5RJ/hdOddcxihQDAgErOXUr473b4zKvxTG05d2BxD9Vvb8FYF57ZkZqaiSpv2TefVPvcO+t87KxLghrQGE3oKFEEHlXllejKeOOx3XD8xBbvfG6N2iDjo2qol/DO+oWf4/l/fQ3Z/IPDumJy7p1RQ9mtcxLuxDzCxuV6mQLbeWn1hts5/eXLs2rYXv7jwDD325Unc5jLzx52DcB3lYss0/S2YoA7C+XrYran+VtFSMG9Iab8zOBwD8fN9QzFi9G+MmL8Y5nRqiU+NaUceMzzUOFLvlzDa45UzjqN3zuzTC/Z9HB4GN7tUMo3vpGx61qPaf7xsa/KyYdRrqIVQSl1H/1vXRPyKgTo/cbo1RLSMVT/2wzkGpvKFVViaeu8J6gF8kXj0ZSdNDkDGaGVQ5fqKJex1W35rJ4An2S9c9PA4hYpvecfJhPrkOxl18k0KzsmF64EqlWCIq10Sd0hgpNyk1qYnyfiU8UlmEbdPDivHwO2zU/E/SGASzVJZufKIqkMippUZvyF6uXGqF0BiCUsknhux2k6SXnRCwQYhA7WH14w/XbBapREPrjV/rem29NfqdEVtOXSGszTKqjC6jynQtjuHjBDmVCmNFWjkw8q37lUjja6Q0E8ZlpJLeMzEkZ5KJBFUb1onnx+dHnWMkkg9FNoVVF5Af1jcyh3ItI3naacIIbytJetkJQdJMO1X2DNbsOoKXft4U/L5s+yHMXLMbtw1ti88W7whuv/mDPNTPzMAnf2ivr+9XUohQPSPxbq+WgteaHWbUQ7AS0OWkolJGKlcEp52ahxfEY9wg8TSGDVw+6few7xe/Mg8AUKdaRtj2mWv2uCZTp8a1sLbwSNRnJZkZqXjpqkDw073ndcBHC7dpno8IuH94B8x92Vw0dY0qaVFR21bp0bwOalZJU43gvn5gTtj3J0Z3Q2aV6OC66wfl4O15W/Dn/oGMV5EKcMLIzpjw7RoAwC1DWqsqyIt6NMGzM9ZjzxFraT5rVknDjYNa4ZLeTS0dF8ltQ9vg7E4Nwrb9e1RX1KmWjrM6NsDj0wPyW1HuDWtVwfUDc3Blv+YY/sKcuORT8o/hHdGxcU3jgjbw+V8G4qulO4PBfZ//ZSB+WuvebywePhzbH0u3HYzp2H+N6oL6mVUAAH87tz36tKxrcEQItyPUk9Jl5MoApQUu6tEE0+8aHPw+5eb+quUeH90NrbIyAQD1MjNUy8ikEKF7M3PRkrndGmNE9+jUhEbcGhHsNXlsP3x4U0j2/CcuRE8pYnNkj/Do5Kv7t8ContGKt2bVdCz753m47/wOqnVeP6hV8PODF3ZSdbtUTU/Ffy63njWLiPDPkZ01s4CZ5f7hHdGnZb2wbY1qV8Uzl/dARlpKTIFpKUSYcFEXdGwUHZwXD38Z2gbDOjQwLmgDXZvWxvgRnYP3rE/LugkTsTy4XRbulNKtWuXaAYEobgC46+x2GNQ2y+AI70gagyAbAeXyw37CS39yRYwNEuneiXTfpKRQ0NUTb9JzqxIGVxf14b2ObS0jhnGe5DEIJjSDX2as2DFl1MqlxG4QIgyASp3lQmjuM0OiTp81g08eN8aHePXcJ5FBUHzWzO3qkjAu1Gs1PWMsMhj1EABATpMcr7H145t+rFQk+Swjxjy8dIVD+FmhRKkFG/SEpRks8VcXqFOl0opgDyHmLgKAyukyYXPA+I3kMQiSSgmsJeOxMBE48aZoPYF7/DKo9hBkgxDjk1YZlSZPIWX8SvIYhDCXkTpe/T4j69VSFM7NqY/RQkZUohZUJs+5j3fNIav5q/2sbIMuIxNPnJ+vg3EOr+57UsQhvDk7PzzxjUK3vP7b5uBneX676zhw8630Oqqk2ZNsR6/KmD1GlVAjypdkai2jynf5jAm8uu1JYRC2HSjR3HfsVLmLkpgj8mEY3qURfli9O6rcpD/3xmPT1qJ+jSo4caoc40d0wn9mbECr+tWDZSaM7IwW9avjl3V7MXnBVtX6/jK0DT7U2AcEpoyaSRIkK+/3b+yHDbuLAQCvX9MHk+dvReusGprHvXBFT0tJiB4e0RmtszNNl/cbk8f2x6d/bEeDmlUMy17YrTEW5h/QjMtg4uPdG07Dlr3HvBYjiluHtsG+oyejAjqdJikMgt/Xu9FzHRRMzMU9nyxV3Te8a2MM7xoeUHZGu+yw73Ig11kdG2LHwRL8sn5v1Hk6NKqpOfW0YGIu9h89iT6P/RS2fViHbNXyAHBm+2yc2T6wv012DUy4qItmWQC4uJe1yOCxg1sZF5Lw23gRALRvWBPjRxhnegMCvbeJUmpPxn6GdWiAYT60tbWqpuPpy6wHV8ZLUowhpERYBL8lLI90C7jtJkkhQnmF9n4v3TY+t+UMU6lIDoPgc0es19KlUOzBaW7hc/EYplKQJAYh9NmPS1dE9RBcr19/jMBLgxVn+ALDMBbw1CAQ0XAiWk9Em4joAafq8XsPwSxOGrJyv1lJhmFcxzODQESpAF4BcAGAzgCuIiJzI23W6wp+FvB/1KsXYwp68/z9YE9jHffx23gRw/gZL3sI/QBsEkLkCyFOAfgEwCgnKoqcZWQ1yMlp/LCAm77LyMtBZfYZMYxbeGkQmgJQpiLbIW2zHaWu21t8EhamvLtC9BiCtbzCZtFbgtqq4k0hcmU6bzCIizU8wziOl3EIar/wKFVNROMAjAOAFi1axFTRkq2xZToyww2DcpCemoI3ZucDALo0qYXVu0LZznq1qIOl2w4BAEZ0b4zzujTCXVPC4wpkpffoRV2QkxUdcPXQhZ1QNT01mGQjVh4f3Q31Mzdgal50StBHR3VBVs0MfLgglIXt2gEtJQFD5W4+oxVOlFbgjrPaIrNKGtYVFqsGzdlF/1b1cN2AlrglIhmPX/j3xV3RrE41V+t8/ooeSIt1cSiG0cHLp2oHgOaK780A7IosJIR4QwjRVwjRNztbOxhKj3jfsLXerBvUrIJHRnbB/13YKbjt9Wv6hJVpmx2K0H356t7I7RZS6o9d3FWWEABw3cAcnNk+O0re+jWq4InR3eJeYqJhrar418XhQWLnSKkeG9aqiscu7hbcXjAxF/8aFZBPlqdKWgoeyu2Mf1/cFQ1rVUWNKmmYeGk3OElaagoeHdUVTWJUuk57B685vSWGdXQn45jM6F7NojLQMYwdeGkQ/gDQjohaEVEGgCsBfONERfHOMtIyCGquJ6O61FJ5ujloG09bqB1bGdcaYphkxTOXkRCijIjuAPAjgFQA7wghVjtRV7y9a+2VOqMtQmTRyBJe689YqpePUbOLfl8WhGEY83i6lpEQYjqA6U7X42UPQddlEVwG2T1iaYtQTyb6WL/GePAgNMNYJylGppwyCGrTVyNLRs6Dj4yJCGyLOIeDuiyWcwcTuqjs86tBYBjGOkliEOI73koPIeotWqeHEFK07inVmHz+Ohm+2B4wTOUhKQxCvAOfWm/Bqj0E8/YgeLxRHILX6CWF97tB8FnICcP4mqQwCNUz4puuWTVdvZnUeg5WXCjystxeul30gtUiSU+Nbge/uozky4o3dSfDJBNJkSDn7rPb4bsVhabLP3VpN/zj85UAArECA9rUx9nP/hZWZnSvpvjL0OhgKaX6uX5gDno0r40vl+5UdVuN6dscG/ccxV/PbR9+Dod12H3ntcfgdtn4aulO3DYs/BqevrQ7smpmhG2rUz0ddwxri1E9o+e++9Ug9M2ph7GDW1lKpsMwyU5SGIR2DWtiSPtszN4Qni1saIds/Lp+L87r3BCX9WmGcZMX45xODXDFaS1wxWnqUdHXD8zRzQAm68daVdMw4aIu+HrZTgCBVIiRVE1Pxb+DwWnuccdZ7QAAPZvXido35rTmUduISDOFo1+nnaamEB42mZWMYZgASeEy0kIeFE4hCvrHjSJbjQPPpPNEbvfpm3S8VNbrYphkJGkMgurCSZL2T0kJ7TcahDTUfxr7rahNVrEMw3hB0hgEPYjItN/etItEsixCZ8omwzCMn0gag6D25i9Pp1S6gYxyJRi5SKKnnVqPRmY3DMMwXpA0BkGNiorA31Qy/wZvtpxsVkI9BFbyDMP4m6Q2CPLbe1gPIc5zRi1dEcMJ2XQwDOMFSW0QZEOQnpqCVGlJ1DSDQQKjKGIKnjN8thEreYZh/E5SxCEA6mMDI7o3QYdGNXH32e1Qs2o6bhrcCuPObK16/P9uHYDLJs3H7cPUM3dNGNkZXZvWRo0qafjrOe1xYbdG4fVasAjsXWIYxguSxiCokZ5KeGRkKMhsvE4gU9+ceiiYmKu5//pBoYjYu89pF/wc6iGwlmcYxt8ktcvIlYHeGKad8gA0wzBekNwGwYU6Ypl2yjAM4wVJbRDcgAPTGIZJFJLaILippHkMgWEYv5PUBsENOEELwzCJQlIbBFfGlNllxDBMgpDUBsGN5C7BQWU2CAzD+Jyki0Ool5mB03Lqolnd6qpJa+wmFA8XsggPj+iMfjn1HK/bLZ68pBtaZ2V6LQbDMHGSdAbhhSt6Ykj7bNfqCwamKXoIlS2t41X91LPLMQyTWCS1y8gVBMchMAyTGLBBcAkeQ2AYxu8kjUGIZRlqW+qV/nIcAsMwfidpDIKM22/qPO2UYZhEIekMgtsIHkNgGCZBSDqD4LbrKDTLiE0CwzD+JmkMQtX0wKWmGmREs5u01EC9cgY1hmEYv5I0cQgTL+2Od+dtwYDW9V2td0zfZthxoAR3nt3OuLCCR0Z2Rv9W7srKMExyQ2qpJf1K3759RV5entdiMAzDJBREtFgI0deoXNK4jBiGYRh92CAwDMMwANggMAzDMBKeGAQimkBEO4lomfTvQi/kYBiGYUJ4OcvoeSHEfzysn2EYhlHALiOGYRgGgLcG4Q4iWkFE7xBRXQ/lYBiGYeCgQSCin4holcq/UQBeA9AGQE8AhQCe1TnPOCLKI6K8vXv3OiUuwzBM0uN5YBoR5QD4TgjR1UTZvQC2xlhVFoB9MR7rJCyXNVgua/hVLsC/slVGuVoKIQxTRXoyqExEjYUQhdLX0QBWmTnOzAXp1JlnJlLPbVgua7Bc1vCrXIB/ZUtmubyaZfQ0EfVEYDHQAgC3eCQHwzAMI+GJQRBCXONFvQzDMIw2yTTt9A2vBdCA5bIGy2UNv8oF+Fe2pJXL80FlhmEYxh8kUw+BYRiG0SEpDAIRDSei9US0iYgecLHe5kT0CxGtJaLVRHS3tF1zLScielCScz0Rne+wfAVEtFKSIU/aVo+IZhLRRulvXWk7EdGLkmwriKi3QzJ1ULTLMiI6QkT3eNFmUtBkERGtUmyz3D5EdJ1UfiMRXeeQXM8Q0Tqp7i+JqI60PYeIjivabZLimD7S/d8kyR5XWj8NuSzfN7t/rxpyTVXIVEBEy6TtbraXln7w7hkTQlTqfwBSAWwG0BpABoDlADq7VHdjAL2lzzUBbADQGcAEAPeplO8syVcFQCtJ7lQH5SsAkBWx7WkAD0ifHwDwlPT5QgDfAyAApwNY6NK92w2gpRdtBmAIgN4AVsXaPgDqAciX/taVPtd1QK7zAKRJn59SyJWjLBdxnkUABkgyfw/gAgfksnTfnPi9qskVsf9ZAP/0oL209INnz1gy9BD6AdgkhMgXQpwC8AmAUW5ULIQoFEIskT4XA1gLoKnOIaMAfCKEOCmE2AJgEwLyu8koAO9Ln98HcLFi+wciwAIAdYioscOynA1gsxBCLxjRsTYTQswGcEClPivtcz6AmUKIA0KIgwBmAhhut1xCiBlCiDLp6wIAzfTOIclWSwgxXwS0ygeKa7FNLh207pvtv1c9uaS3/DEApuidw6H20tIPnj1jyWAQmgLYrvi+A/pK2REoEJHdC8BCaZPaWk5uyyoAzCCixUQ0TtrWUEhBg9LfBh7JBgBXIvyH6oc2s9o+XrTbjQi8Scq0IqKlRPQbEZ0hbWsqyeKGXFbum9vtdQaAPUKIjYptrrdXhH7w7BlLBoOg5udzdWoVEdUA8DmAe4QQR6C9lpPbsg4SQvQGcAGA24loiE5ZV2UjogwAFwH4TNrklzbTQksOt9vtIQBlAD6SNhUCaCGE6AXgbwA+JqJaLspl9b65fT+vQvhLh+vtpaIfNItqyGCbbMlgEHYAaK743gzALrcqJ6J0BG72R0KILwBACLFHCFEuhKgA8CZCLg5XZRVC7JL+FgH4UpJjj+wKkv4WeSEbAkZqiRBijySjL9oM1tvHNfmkwcQRAP4kuTUguWT2S58XI+Cfby/JpXQrOSJXDPfNzfZKA3AJgKkKeV1tLzX9AA+fsWQwCH8AaEdEraS3zisBfONGxZJ/8m0Aa4UQzym2K33vyrWcvgFwJRFVIaJWANohMJDlhGyZRFRT/ozAoOQqSQZ5lsJ1AL5WyHatNNPhdACHRWg9KicIe3PzQ5sp6rPSPj8COI+I6krukvOkbbZCRMMB/APARUKIEsX2bCJKlT63RqB98iXZionodOk5vVZxLXbKZfW+ufl7PQfAOiFE0BXkZntp6Qd4+YzFM0qeKP8QGJ3fgIC1f8jFegcj0HVbAWCZ9O9CAJMBrJS2fwOgseKYhyQ51yPOWQwGsrVGYAbHcgCr5XYBUB/ALAAbpb/1pO0E4BVJtpUA+jooW3UA+wHUVmxzvc0QMEiFAEoReAsbG0v7IODT3yT9u8EhuTYh4EeWn7NJUtlLpfu7HMASACMV5+mLgILeDOBlSIGqNstl+b7Z/XtVk0va/h6AWyPKutleWvrBs2eMI5UZhmEYAMnhMmIYhmFMwAaBYRiGAcAGgWEYhpFgg8AwDMMAYIPAMAzDSLBBYJIaIiqn8NVVdVfXJKJbiehaG+otIKKseM/DMHbC006ZpIaIjgohanhQbwEC88j3uV03w2jBPQSGUUF6g3+KiBZJ/9pK2ycQ0X3S57uIaI20cNsn0rZ6RPSVtG0BEXWXttcnohnSommvQ7H+DBH9WapjGRG9LkfKMozbsEFgkp1qES6jKxT7jggh+iEQlfqCyrEPAOglhOgO4FZp26MAlkrb/g+BZZIB4BEAc0Vg0bRvALQAACLqBOAKBBYa7AmgHMCf7L1EhjFHmtcCMIzHHJcUsRpTFH+fV9m/AsBHRPQVgK+kbYMRWP4AQoifpZ5BbQSStFwibZ9GRAel8mcD6APgj8DSNqiG0GJmDOMqbBAYRhuh8VkmFwFFfxGAh4moC/SXIlY7BwF4XwjxYDyCMowdsMuIYbS5QvF3vnIHEaUAaC6E+AXA/QDqAKgBYDYklw8RDQWwTwTWuFduvwCBVIdAYPGyy4iogbSvHhG1dPCaGEYT7iEwyU41khKsS/wghJCnnlYhooUIvDhdFXFcKoAPJXcQAXheCHGIiCYAeJeIVgAoQWgZ40cBTCGiJQB+A7ANAIQQa4hoPAKZ61IQWJHzdgB6aUMZxhF42inDqMDTQplkhF1GDMMwDADuITAMwzAS3ENgGIZhALBBYBiGYSTYIDAMwzAA2CAwDMMwEmwQGIZhGABsEBiGYRiJ/wc3ZmgUQtDVlAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f2fab797128>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.plot()\n",
    "plt.plot(range(len(scores)), scores)\n",
    "plt.xlabel(\"Episode\") \n",
    "plt.ylabel(\"Total Reward\")\n",
    "plt.savefig(\"save\\\\reward.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
